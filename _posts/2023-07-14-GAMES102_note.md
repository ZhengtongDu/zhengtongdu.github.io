---
layout: post
title: GAMES102_note
date: 2023-07-14 09:40 +0800
tags: ["Learning Notes", "GAMES102"]
toc: true
---

> GAMES102-几何建模与处理 Geometry Modeling and Processing
> 这个笔记的主要内容分为两部分，一部分是对于课程内容脉络的梳理，整理我最后记住的每个章节的内容；另一部分是我觉得更重要的，是关于一些零散知识点的整理，这里涉及到的知识点结合了自己过往的学习内容，纳入了自己的思考。

## 数据拟合

### 基函数的选取与秦九韶算法

在考虑数值逼近（插值或拟合）问题的时候，合适的基函数是非常重要的，按Weierstrass定理，任意闭区间上的连续函数都可以用多项式级数一致逼近。但是在进行插值的过程中，选取什么形式的多项式作为基函数值得考虑。

显然，由{ $1, x, x^2, \cdots, x^n$ }是可以张成一个维数为 $n+1$ 的多项式函数空间的，但是为什么我们不用这种方法来计算插值呢？主要原因以下几点

- 计算开销上的困难：虽然求解系数的线性方程组的系数矩阵是Vandermonde矩阵，其本身是非退化的，但是Vandermonde矩阵的条件数增长速度非常快， $n$ 阶的Vandermonde矩阵的条件数至少是 $2^{n-2}$ ，带来的计算开销过大了；
- 高阶多项式插值带来的Runge现象，这其实从根源上限制了 $n$ 必须非常小。

因为这些原因，我们放弃了直接以幂函数作为基函数进行多项式插值的方法，寻求其他办法（Lagrange插值或Newton插值）。

此外，怎么通过计算机计算 $a_0 + a_1x + a_2x^2 + \dots + a_nx^n$ ，答案是秦九韶算法：

$$
    sum = (((a_nx + a_{n - 1})x + a_{n - 2})x + \dots + a_1)x + a_0
$$

相比于直接计算，秦九韶算法需要更少的计算次数，用常规方法计算出结果最多需要 $n$ 次加法和 $\frac{n^2+n}{2}$ 次乘法，而秦九韶算法最多只需 $n$ 次加法和 $n$ 次乘法。

### 从基函数到RBF函数到神经网络

在人工神经网络的数学理论中，通用近似定理（万能近似定理）意味着神经网络可以用来近似任意的复杂函数，并且可以达到任意近似精准度。但它并没有说明要如何选择神经网络参数（权重、神经元数量、神经层层数等）来达到想近似的目标函数。这其实可以看做是Weierstrass定理的一个推广。

常用的用来搭建神经网络的底层函数包括了径向基函数，其中最常用的是Gauss函数。由于其具有可线性组合及局部支撑的性质，因此通过调参可以很好地拟合高维函数。

虽然神经网络是一个比较新颖的领域，但是它的本质就是处理优化问题，和传统的数值逼近在思想上是一致的，但神经网络进行优化的目标与传统逼近的目标是不同的（二者对于精度的要求和保证强度不一样），由此产生了差异。

## 参数曲线拟合